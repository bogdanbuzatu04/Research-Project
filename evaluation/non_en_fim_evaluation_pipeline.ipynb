{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3BwwXPL012Jy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088430737,
     "user_tz": -120,
     "elapsed": 3635,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    },
    "outputId": "113fd529-c9aa-4f01-80fb-f9726e6d7c88"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein) (3.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9y3ErhV81_zH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088433991,
     "user_tz": -120,
     "elapsed": 3247,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    },
    "outputId": "24840c1d-353f-4c60-897e-3e10444b1357"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "mQnpUiMJ2GvB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088433998,
     "user_tz": -120,
     "elapsed": 5,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    },
    "outputId": "74e1c79d-c825-4273-adf0-e37edcf6d3f8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tree_sitter_java\n",
    "from tree_sitter import Language, Parser, Query\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList, StoppingCriteria\n",
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "random.seed(42)\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Y8mLSvSf4U0R",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434044,
     "user_tz": -120,
     "elapsed": 1,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "PBZF7gvM4Xky",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434895,
     "user_tz": -120,
     "elapsed": 842,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    },
    "outputId": "dc55188e-994e-4efe-f8e0-7011ce2feca2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "tUW5WI7J4iyX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434897,
     "user_tz": -120,
     "elapsed": 4,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/content/drive/MyDrive/Personal_RP/heap_Java_sampled_non_english_FIM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "sZbHaRjW4z5H",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434899,
     "user_tz": -120,
     "elapsed": 1,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "mfrehOLM5Epx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434912,
     "user_tz": -120,
     "elapsed": 11,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "ds = load_from_disk(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ENVYvrWK-5DC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434913,
     "user_tz": -120,
     "elapsed": 1,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "parser = Parser(JAVA_LANGUAGE)\n",
    "COMMENT_JAVA_QUERY = JAVA_LANGUAGE.query(\n",
    "    \"\"\"\n",
    "    (block_comment)       @comment.block\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "F2w5PIwC_xVT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434916,
     "user_tz": -120,
     "elapsed": 2,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "GNeP-N-T_QM4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088434918,
     "user_tz": -120,
     "elapsed": 1,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "def parse_code(input_code):\n",
    "    encoded_code = bytes(input_code, 'utf8')\n",
    "    tree = parser.parse(encoded_code)\n",
    "    root_node = tree.root_node\n",
    "\n",
    "\n",
    "    captures: dict[str, list] = COMMENT_JAVA_QUERY.captures(root_node)\n",
    "    # ← this is a bytes object\n",
    "    results = []\n",
    "\n",
    "    for cap_name, nodes in captures.items():\n",
    "        for node in nodes:\n",
    "            start = node.start_byte\n",
    "            end = node.end_byte\n",
    "\n",
    "            c_start = len(encoded_code[:start].decode('utf-8', errors='replace'))\n",
    "            c_end = len(encoded_code[:end].decode('utf-8', errors='replace'))\n",
    "            snippet = input_code[c_start:c_end]\n",
    "\n",
    "            results.append(\n",
    "                (cap_name, c_start, c_end, snippet)\n",
    "            )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "qisy4oUEAieW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088439185,
     "user_tz": -120,
     "elapsed": 4261,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU \"transformers>=4.41.0\" accelerate bitsandbytes sentencepiece safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "dRNHcFD5uqJv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088439198,
     "user_tz": -120,
     "elapsed": 10,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MODELS = {\n",
    "        'mellum_base_4b': {\n",
    "            'path': 'JetBrains/Mellum-4b-base',\n",
    "            'context_length': 2048,\n",
    "            'fim_prefix': '<fim_prefix>',\n",
    "            'fim_suffix': '<fim_suffix>',\n",
    "            'fim_middle': '<fim_middle>',\n",
    "        },\n",
    "        'smol_lm_135m': {\n",
    "            'path': 'HuggingFaceTB/SmolLM-135M',\n",
    "            'context_length': 2048,\n",
    "        },\n",
    "        'starcoder2_3b': {\n",
    "            'path': 'bigcode/starcoder2-3b',\n",
    "            'context_length': 2048,\n",
    "            'fim_prefix': '<fim_prefix>',\n",
    "            'fim_suffix': '<fim_suffix>',\n",
    "            'fim_middle': '<fim_middle>',\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "vp0a5FydQa6m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088439201,
     "user_tz": -120,
     "elapsed": 2,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "def find_comment_body(\n",
    "    c_start: int,\n",
    "    c_end:   int,\n",
    "    comment: str\n",
    ") -> Tuple[int,int]:\n",
    "    \"\"\"\n",
    "    Given comment = code[c_start:c_end], verify it has an opening\n",
    "    '/*' or '/**' and a closing '*/'.  If so, return the absolute\n",
    "    (body_start, body_end) positions in the code string *excluding*\n",
    "    those markers; otherwise return c_end, c_end.\n",
    "    \"\"\"\n",
    "    # quick strip check\n",
    "    if not (comment.startswith(\"/*\") and comment.endswith(\"*/\")):\n",
    "        return c_end, c_end\n",
    "\n",
    "    # determine how many chars to strip off the front\n",
    "    if comment.startswith(\"/**\"):\n",
    "        lead = 3\n",
    "    else:  # must be \"/*\"\n",
    "        lead = 2\n",
    "\n",
    "    trail = 0\n",
    "\n",
    "    # compute absolute positions in the full code\n",
    "    body_start = c_start + lead\n",
    "    body_end   = c_end   - trail\n",
    "\n",
    "    # sanity check\n",
    "    if body_start >= body_end:\n",
    "        return c_end, c_end\n",
    "\n",
    "    return body_start, body_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "iCAKslof8s1D",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088439210,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "def estimate_target_tokens_comment_body(\n",
    "    ds: Dataset,\n",
    "    tokenizer_name: str,\n",
    "    parse_fn,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Scan the dataset once to estimate mean+2*std of token lengths\n",
    "    for all parsed snippets (e.g. comments).\n",
    "    Returns the ceiling of mean + 2*std.\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
    "    lengths = []\n",
    "\n",
    "    for sample in ds:\n",
    "        code = sample[\"content\"]\n",
    "        annotations = sample.get(\"language_detected\", [])\n",
    "        for _, c_start, c_end, _ in parse_fn(code):\n",
    "            # slice out the per-char language tags for this comment\n",
    "            comment_ann = annotations[c_start:c_end]\n",
    "            # drop any “-1” (undetected) labels\n",
    "            langs = [l for l in comment_ann if l != \"-1\"]\n",
    "            if not langs:\n",
    "                continue\n",
    "\n",
    "            # find the single most‐common language in the span\n",
    "            top_lang, _ = Counter(langs).most_common(1)[0]\n",
    "            if top_lang == \"en\":\n",
    "                continue  # skip English comments\n",
    "\n",
    "            # grab the comment\n",
    "            comment = code[c_start:c_end]\n",
    "            comment_body_start, comment_body_end = find_comment_body(c_start, c_end, comment)\n",
    "\n",
    "            if comment_body_start == comment_body_end:\n",
    "              # malformed comment like \"/*oops\" or missing closing \"*/\"\n",
    "              continue\n",
    "\n",
    "            snippet = code[comment_body_start:comment_body_end]\n",
    "            ids = tok(snippet, return_attention_mask=False)[\"input_ids\"]\n",
    "            lengths.append(len(ids))\n",
    "\n",
    "    mean, std = np.mean(lengths), np.std(lengths)\n",
    "    return int(np.ceil(mean + 2 * std))\n",
    "\n",
    "\n",
    "def get_context_size(\n",
    "    *,\n",
    "    context_length: int,\n",
    "    target_tokens: int,\n",
    "    supports_fim: bool\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Given a model's context window and a fixed target size,x\n",
    "    return (prefix_tokens, suffix_tokens).\n",
    "    If supports_fim=False, suffix_tokens will be 0.\n",
    "    \"\"\"\n",
    "    if supports_fim:\n",
    "        rem = context_length - target_tokens\n",
    "        pre = rem // 2\n",
    "        suf = rem - pre\n",
    "    else:\n",
    "        pre, suf = context_length - target_tokens, 0\n",
    "\n",
    "    return pre, suf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "2Wx5JK2wBaro",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088440963,
     "user_tz": -120,
     "elapsed": 1753,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "target_size = estimate_target_tokens_comment_body(ds,Config.MODELS['mellum_base_4b']['path'],parse_fn=parse_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "rXJ5EcYqTJpL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088440967,
     "user_tz": -120,
     "elapsed": 2,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "prefix, suffix = get_context_size(\n",
    "    context_length = Config.MODELS['mellum_base_4b']['context_length'],\n",
    "    target_tokens  = target_size,\n",
    "    supports_fim   = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class FIMInput:\n",
    "    def __init__(self, FIM_PREFIX = '<fim_prefix>', FIM_SUFFIX = '<fim_suffix>', FIM_MIDDLE = '<fim_middle>'):\n",
    "        self.FIM_PREFIX = FIM_PREFIX\n",
    "        self.FIM_SUFFIX = FIM_SUFFIX\n",
    "        self.FIM_MIDDLE = FIM_MIDDLE\n",
    "\n",
    "    def generate(self, query_tuple: Tuple[str, str, str]):\n",
    "        prefix = query_tuple[0]\n",
    "        suffix = query_tuple[1]\n",
    "        middle = query_tuple[2]\n",
    "\n",
    "        text = self.FIM_PREFIX + prefix + self.FIM_SUFFIX + suffix + self.FIM_MIDDLE\n",
    "        return text, middle"
   ],
   "metadata": {
    "id": "l_t2RsGF0QqS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088440972,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fim_input = FIMInput()"
   ],
   "metadata": {
    "id": "YuxjNKTg0Sit",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088440973,
     "user_tz": -120,
     "elapsed": 0,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "NMoF1L9pdM94",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088440976,
     "user_tz": -120,
     "elapsed": 2,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "def make_target_context(ds: Dataset, prefix_size: int, suffix_size: int, parse_fn):\n",
    "\n",
    "    context_target_pair = []\n",
    "    for sample in ds:\n",
    "        code = sample[\"content\"]\n",
    "        annotations = sample.get(\"language_detected\", [])\n",
    "\n",
    "        for _, c_start, c_end, _ in parse_fn(code):\n",
    "            # slice out the per-char language tags for this comment\n",
    "            comment_ann = annotations[c_start:c_end]\n",
    "\n",
    "            # drop any “-1” (undetected) labels\n",
    "            langs = [l for l in comment_ann if l != \"-1\"]\n",
    "            if not langs:\n",
    "                continue\n",
    "\n",
    "            # find the single most‐common language in the span\n",
    "            top_lang, _ = Counter(langs).most_common(1)[0]\n",
    "\n",
    "            if top_lang == \"en\":\n",
    "                continue  # skip English comments\n",
    "\n",
    "\n",
    "            # grab the comment\n",
    "            comment = code[c_start:c_end]\n",
    "\n",
    "            comment_body_start, comment_body_end = find_comment_body(c_start, c_end, comment)\n",
    "\n",
    "            if comment_body_start == comment_body_end:\n",
    "              # malformed comment like \"/*oops\" or missing closing \"*/\"\n",
    "              continue\n",
    "\n",
    "\n",
    "\n",
    "            prefix  = code[max(0, comment_body_start - prefix_size) : comment_body_start]\n",
    "            suffix = code[comment_body_end : min(len(code), comment_body_end + suffix_size)]\n",
    "            target = code[comment_body_start : comment_body_end]\n",
    "\n",
    "\n",
    "            if (prefix and suffix) and target:\n",
    "                context_target_pair.append(fim_input.generate((prefix, suffix, target)))\n",
    "\n",
    "    return context_target_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "EP9mHO4ig-nM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088442078,
     "user_tz": -120,
     "elapsed": 1095,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "context_target_pair = make_target_context(ds, prefix, suffix, parse_fn=parse_code)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "context_target_pair[6]"
   ],
   "metadata": {
    "id": "uEy8_9Zg0atl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088442085,
     "user_tz": -120,
     "elapsed": 5,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    },
    "outputId": "30421edd-0d88-48ba-9f51-0c82f7bf712f"
   },
   "execution_count": 63,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('<fim_prefix>package net.cocotea.elysiananime.common.enums;\\n\\nimport lombok.AllArgsConstructor;\\nimport lombok.Getter;\\n\\n/**<fim_suffix>\\n@Getter\\n@AllArgsConstructor\\npublic enum SexEnum {\\n    /**\\n     * 未知\\n     */\\n    UNKNOWN(0, \"未知\"),\\n    /**\\n     * 男\\n     */\\n    MAN(1, \"男\"),\\n    /**\\n     * 女\\n     */\\n    WOMAN(2, \"女\");\\n\\n    final Integer code;\\n    final String desc;\\n}\\n<fim_middle>',\n",
       " '\\n * 性别枚举值\\n *\\n * @author CoCoTea\\n * @version 2.0.0\\n */')"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFgAA7WKSH5V"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "wQj53Gz3zDY6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088442087,
     "user_tz": -120,
     "elapsed": 1,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "    def __init__(self, config, device, local=False):\n",
    "        self.device = device\n",
    "        repo = config['path_local'] if local else config['path']\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(str(repo), local_files_only=local)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(str(repo), local_files_only=local).to(device)\n",
    "        self.context_length = config['context_length']\n",
    "\n",
    "        # initialize FIM tool if tokens provided\n",
    "        if all(k in config for k in ('fim_prefix', 'fim_suffix', 'fim_middle')):\n",
    "            self.fim_tool = FIMInput(\n",
    "                FIM_PREFIX = config['fim_prefix'], FIM_SUFFIX = config['fim_suffix'], FIM_MIDDLE =config['fim_middle']\n",
    "            )\n",
    "        else:\n",
    "            self.fim_tool = None\n",
    "\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens: int, attention_mask=None, stopping_criteria=None, pad_token_id=None, eos_token_id=None) -> torch.LongTensor:\n",
    "        return self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            attention_mask=attention_mask,\n",
    "            # temperature=0.0,\n",
    "            do_sample = False,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "6LmQISTAzJrv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088442089,
     "user_tz": -120,
     "elapsed": 1,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "class StopOnLineGenerated(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, offset):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.offset = offset\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        ids = input_ids[0][self.offset:]\n",
    "        pred_text = self.tokenizer.decode(ids, skip_special_tokens = True)\n",
    "        if first_line(pred_text):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class StopOnCommentGenerated(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, offset):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.offset = offset\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        ids = input_ids[0][self.offset:]\n",
    "        pred_text = self.tokenizer.decode(ids, skip_special_tokens = True)\n",
    "        return '*/' in pred_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "XrvkyWuzzPWN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088442091,
     "user_tz": -120,
     "elapsed": 1,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "class PipelineTool:\n",
    "    def __init__(self, model, fin_mask_token: str = None):\n",
    "        self.model = model\n",
    "\n",
    "    def  task_line_completion(self, context: str, line: str, max_tokens: int = 160) -> Tuple[List[int], List[str]]:\n",
    "\n",
    "\n",
    "        context_tokens = self.model.tokenizer(context, return_tensors='pt').to(self.model.device)\n",
    "        context_ids = context_tokens['input_ids'][0][-self.model.context_length:].unsqueeze(0)\n",
    "        context_attention_mask = context_tokens['attention_mask'][0][-self.model.context_length:].unsqueeze(0)\n",
    "\n",
    "\n",
    "        line_tokenized = self.model.tokenizer(line, return_tensors='pt').to(self.model.device)\n",
    "        line_ids = line_tokenized['input_ids'][0].unsqueeze(0)\n",
    "        line_mask = line_tokenized['attention_mask']\n",
    "\n",
    "        stopping_criterion = StopOnLineGenerated(self.model.tokenizer, context_ids.shape[-1])\n",
    "        stopping_criteria = StoppingCriteriaList([stopping_criterion])\n",
    "\n",
    "        gen = self.model.generate(\n",
    "            input_ids=context_ids,\n",
    "            attention_mask=context_attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=self.model.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.model.tokenizer.pad_token_id,\n",
    "            stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        truth_ids = line_ids[0]\n",
    "\n",
    "        context_ids = context_ids.detach().cpu()\n",
    "        context_attention_mask = context_attention_mask.detach().cpu()\n",
    "\n",
    "        generated_line_ids = gen[0][context_ids.shape[-1]:]\n",
    "\n",
    "        truth_toks = self.model.tokenizer.convert_ids_to_tokens(truth_ids, skip_special_tokens = True)\n",
    "        pred_toks = self.model.tokenizer.convert_ids_to_tokens(generated_line_ids, skip_special_tokens = True)\n",
    "\n",
    "        truth_text = self.model.tokenizer.decode(truth_ids, skip_special_tokens = True)\n",
    "        pred_text = self.model.tokenizer.decode(generated_line_ids, skip_special_tokens = True)\n",
    "\n",
    "        gen = gen.detach().cpu()\n",
    "        line_ids = line_ids.detach().cpu()\n",
    "        line_mask = line_mask.detach().cpu()\n",
    "\n",
    "        generated_line_ids = np.array(generated_line_ids.detach().cpu().tolist())\n",
    "        truth_ids = np.array(truth_ids.detach().cpu().tolist())\n",
    "\n",
    "        del context_ids\n",
    "        del context_attention_mask\n",
    "        del gen\n",
    "        del line_ids\n",
    "        del line_mask\n",
    "\n",
    "        return truth_ids, generated_line_ids, truth_toks, pred_toks, truth_text, pred_text\n",
    "\n",
    "    def task_fim(self, context: str, target: str, max_tokens:int = 1500):\n",
    "        context_tokenized = self.model.tokenizer(context, return_tensors = 'pt').to(self.model.device)\n",
    "        context_ids = context_tokenized['input_ids']\n",
    "        context_mask = context_tokenized['attention_mask']\n",
    "\n",
    "        target_tokenized = self.model.tokenizer(target, return_tensors = 'pt').to(self.model.device)\n",
    "        target_ids = target_tokenized['input_ids']\n",
    "        target_mask = target_tokenized['attention_mask']\n",
    "\n",
    "        stopping_criterion = StopOnCommentGenerated(self.model.tokenizer, context_ids.shape[-1])\n",
    "        stopping_criteria = StoppingCriteriaList([stopping_criterion])\n",
    "\n",
    "        gen = self.model.generate(\n",
    "            input_ids=context_ids,\n",
    "            attention_mask=context_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=self.model.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.model.tokenizer.pad_token_id,\n",
    "            stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        pred_ids = gen[0][context_ids.shape[-1]:]\n",
    "        pred_toks = self.model.tokenizer.convert_ids_to_tokens(pred_ids, skip_special_tokens = True)\n",
    "        pred_text = self.model.tokenizer.decode(pred_ids, skip_special_tokens = True)\n",
    "\n",
    "        truth_ids = target_ids[0]\n",
    "        truth_toks = self.model.tokenizer.convert_ids_to_tokens(truth_ids, skip_special_tokens = True)\n",
    "        truth_text = self.model.tokenizer.decode(truth_ids, skip_special_tokens = True)\n",
    "\n",
    "        pred_ids = np.array(pred_ids.detach().cpu().tolist())\n",
    "        truth_ids = np.array(truth_ids.detach().cpu().tolist())\n",
    "\n",
    "        context_ids = context_ids.detach().cpu()\n",
    "        context_mask = context_mask.detach().cpu()\n",
    "        target_ids = target_ids.detach().cpu()\n",
    "        target_mask = target_mask.detach().cpu()\n",
    "        gen = gen.detach().cpu()\n",
    "\n",
    "        del context_ids\n",
    "        del context_mask\n",
    "        del target_ids\n",
    "        del target_mask\n",
    "        del gen\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        return truth_ids, pred_ids, truth_toks, pred_toks, truth_text, pred_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "SscSsz18-agr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1750088442092,
     "user_tz": -120,
     "elapsed": 0,
     "user": {
      "displayName": "Bogdan Buzatu",
      "userId": "02924170350880839349"
     }
    }
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def sentence_bleu(pred_text: List[str], ref_text: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Compute the BLEU score for a single sentence.\n",
    "        Args:\n",
    "            pred_text (List[str]): List of predicted tokens.\n",
    "            ref_text (List[str]): List of reference tokens.\n",
    "        Returns:\n",
    "            float: BLEU score.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chencherry = SmoothingFunction()\n",
    "            return sentence_bleu([ref_text], pred_text,smoothing_function = chencherry.method2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in sentence_bleu: {e}\")\n",
    "            return np.nan\n",
    "\n",
    "    @staticmethod\n",
    "    def lev_total(preds: List[str], refs: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Compute the total Levenshtein distance between two lists of strings. This is the same as edit distance.\n",
    "        Args:\n",
    "            preds (List[str]): List of predicted strings.\n",
    "            refs (List[str]): List of reference strings.\n",
    "        Returns:\n",
    "            float: Total Levenshtein distance (normalized per character).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            length = max(len(preds), len(refs))\n",
    "            preds_p = preds + [\"\"] * (length - len(preds))\n",
    "            refs_p  = refs  + [\"\"] * (length - len(refs))\n",
    "\n",
    "            total_dist = sum(Levenshtein.distance(p, r) for p, r in zip(preds_p, refs_p))\n",
    "            total_chars = sum(max(len(p), len(r)) for p, r in zip(preds_p, refs_p))\n",
    "            return total_dist / total_chars if total_chars > 0 else 0.0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in lev_total: {e}\")\n",
    "            return np.nan\n",
    "\n",
    "    @staticmethod\n",
    "    def meteor_score(pred_tokens: List[str], ref_tokens: List[str]) -> float:\n",
    "        try:\n",
    "            return meteor_score([ref_tokens], pred_tokens)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in meteor_score: {e}\")\n",
    "            return np.nan\n",
    "\n",
    "    @staticmethod\n",
    "    def rouge_score(pred_text: str, ref_text: str) -> List[float]:\n",
    "        try:\n",
    "            rouge = Rouge()\n",
    "            rouge_scores = rouge.get_scores(pred_text, ref_text)\n",
    "            res = [\n",
    "                rouge_scores[0]['rouge-1']['p'],\n",
    "                rouge_scores[0]['rouge-1']['r'],\n",
    "                rouge_scores[0]['rouge-1']['f'],\n",
    "                rouge_scores[0]['rouge-2']['p'],\n",
    "                rouge_scores[0]['rouge-2']['r'],\n",
    "                rouge_scores[0]['rouge-2']['f'],\n",
    "                rouge_scores[0]['rouge-l']['p'],\n",
    "                rouge_scores[0]['rouge-l']['r'],\n",
    "                rouge_scores[0]['rouge-l']['f'],\n",
    "            ]\n",
    "            return res\n",
    "        except Exception as e:\n",
    "            print(f\"Error in rouge_score: {e}\")\n",
    "            return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "    def exact_match(generated_ids: np.ndarray, ground_truth_ids: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute exact match accuracy between two 1D numpy arrays.\n",
    "        Pads the shorter one with -1 so lengths match.\n",
    "        Returns a float ∈ [0, 1].\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if generated_ids.ndim != 1 or ground_truth_ids.ndim != 1:\n",
    "                raise ValueError(\"Both inputs must be 1D arrays.\")\n",
    "\n",
    "            len_gt = ground_truth_ids.shape[0]\n",
    "            len_gen = generated_ids.shape[0]\n",
    "\n",
    "            if len_gt != len_gen:\n",
    "                target_len = max(len_gt, len_gen)\n",
    "                pad_gt = np.full((target_len - len_gt,), -1, dtype=ground_truth_ids.dtype)\n",
    "                pad_gen = np.full((target_len - len_gen,), -1, dtype=generated_ids.dtype)\n",
    "                ground_truth_ids = np.concatenate([ground_truth_ids, pad_gt])\n",
    "                generated_ids = np.concatenate([generated_ids, pad_gen])\n",
    "\n",
    "            matches = np.sum(ground_truth_ids == generated_ids)\n",
    "            total = ground_truth_ids.shape[0]\n",
    "\n",
    "            return matches / total if total > 0 else 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error in exact_match: {e}\")\n",
    "            return np.nan\n",
    "\n",
    "    @staticmethod\n",
    "    def get_results(pred_ids: torch.Tensor, pred_tokens: List[str], pred_text: str,\n",
    "                    ref_ids: torch.Tensor, ref_tokens: List[str], ref_text: str, device: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute evaluation metrics and return as a tuple of 13 floats.\n",
    "        \"\"\"\n",
    "        exact_match = Evaluator.exact_match(pred_ids, ref_ids)\n",
    "        levenshtein_distance = Evaluator.lev_total(pred_tokens, ref_tokens)\n",
    "        meteor = Evaluator.meteor_score(pred_tokens, ref_tokens)\n",
    "        rouge_scores = Evaluator.rouge_score(pred_text, ref_text)\n",
    "        sentence_bleu = Evaluator.sentence_bleu(pred_tokens, ref_tokens)\n",
    "\n",
    "        return np.array([\n",
    "            exact_match,\n",
    "            sentence_bleu,\n",
    "            levenshtein_distance,\n",
    "            meteor,\n",
    "            *rouge_scores\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_results() -> List[float]:\n",
    "        \"\"\"\n",
    "        Return a list of default results (all NaN).\n",
    "        \"\"\"\n",
    "        return np.array([np.nan] * 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oiVw7_d8jx8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "410d148777444bbb9d88bf7b6b2d12bc",
      "65c919673eda4418b807b54c578d8e3a",
      "1b6af8a1c31c4696bd34d6974ff7e09f",
      "ed1416c394e14c70b88171574c43291e",
      "944581cf89ad4e0ab91fc0995276398a",
      "efe60a0509c64bea92308068d8d9034a",
      "a7bd8b3ad2d64c00887e9da00f2e092b",
      "6b52b7c854364057b49c3d175da3e6a5",
      "b4c659a053204483be654a6499ad741f",
      "2d994e406b504632a4be188664f3bc9d",
      "5649607ad89b4a78b4d69af2a6475c74"
     ]
    },
    "outputId": "28c3506d-e2c5-4bf6-ce35-2cc7cf67a351"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "410d148777444bbb9d88bf7b6b2d12bc"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "model = ModelWrapper(Config.MODELS[\"mellum_base_4b\"], \"cuda\")\n",
    "tool = PipelineTool(model)\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eGRyzeN93aW"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "evaluation_results = []\n",
    "ctx_len = model.context_length\n",
    "tok     = model.tokenizer\n",
    "\n",
    "for i, (context, target) in enumerate(context_target_pair):\n",
    "    tok_out = tok(context, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    ids     = tok_out[\"input_ids\"][0]\n",
    "    used    = ids.size(0)\n",
    "\n",
    "    target_ids = tok(target, return_attention_mask=False)[\"input_ids\"]\n",
    "    target_len   = len(target_ids)        # this is the scalar you need\n",
    "\n",
    "\n",
    "    if used >= ctx_len:\n",
    "        ids    = ids[-(ctx_len - 1):]\n",
    "        context = tok.decode(ids, skip_special_tokens=True)\n",
    "        used   = ids.size(0)\n",
    "\n",
    "    max_new = min((ctx_len - used), target_len)\n",
    "\n",
    "    if max_new <= 0:\n",
    "        # nothing to generate\n",
    "        continue\n",
    "    try:\n",
    "      truth_ids,  pred_ids, truth_tokens, pred_tokens, truth_text, pred_text = \\\n",
    "          tool.task_fim(\n",
    "              context   = context,\n",
    "              target      = target,\n",
    "              max_tokens= max_new,\n",
    "          )\n",
    "\n",
    "      eval_res = evaluator.get_results(\n",
    "                  pred_ids, pred_tokens, pred_text,\n",
    "                  truth_ids, truth_tokens, truth_text,\n",
    "                  tool.model.device\n",
    "              )\n",
    "\n",
    "      evaluation_results.append(eval_res)\n",
    "      results.append({\n",
    "          \"context\":     context,\n",
    "          \"truth\": truth_text,\n",
    "          \"prediction\": pred_text,\n",
    "      })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] generation/eval failed:\", str(e))\n",
    "        evaluation_results.append(Evaluator.get_default_results())\n",
    "        results.append({\n",
    "            \"prefix\":      None,\n",
    "            \"truth_line\":  None,\n",
    "            \"prediction\":  None,\n",
    "        })\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deA8f688AbQI"
   },
   "outputs": [],
   "source": [
    "for r in results[:40]:\n",
    "    print(\"─── context ───\")\n",
    "    print(r[\"context\"][-200:])       # show the tail of your context\n",
    "    print(\"─── truth ───\")\n",
    "    print(r[\"truth\"].rstrip())\n",
    "    print(\"─ prediction ─\")\n",
    "    print(r[\"prediction\"].rstrip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RFrCYCwdxUY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqrN6i2rdj2Q"
   },
   "outputs": [],
   "source": [
    "metric_names = [\n",
    "    \"exact_match\", \"bleu\", \"levenshtein\", \"meteor\",\n",
    "    \"rouge1_p\",\"rouge1_r\",\"rouge1_f\",\n",
    "    \"rouge2_p\",\"rouge2_r\",\"rouge2_f\",\n",
    "    \"rougeL_p\",\"rougeL_r\",\"rougeL_f\",\n",
    "]\n",
    "metrics_rows = [row.tolist() for row in evaluation_results]\n",
    "\n",
    "df_main    = pd.DataFrame(results)\n",
    "df_metrics = pd.DataFrame(metrics_rows, columns=metric_names)\n",
    "\n",
    "text_path   = \"/content/drive/MyDrive/mellum_non_en_fim_with_results.csv\"\n",
    "metric_path = \"/content/drive/MyDrive/mellum_non_en_fim_with_metrics.csv\"\n",
    "\n",
    "df_main.to_csv(text_path,   index=False, encoding=\"utf8\")\n",
    "df_metrics.to_csv(metric_path, index=False, encoding=\"utf8\")\n",
    "\n",
    "print(f\"Saved {len(df_main)} text results\")\n",
    "print(f\"Saved {len(df_metrics)} metrics results\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "authorship_tag": "ABX9TyN4Ti/fqJJJ4yt6EcpnLeLf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "410d148777444bbb9d88bf7b6b2d12bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_65c919673eda4418b807b54c578d8e3a",
       "IPY_MODEL_1b6af8a1c31c4696bd34d6974ff7e09f",
       "IPY_MODEL_ed1416c394e14c70b88171574c43291e"
      ],
      "layout": "IPY_MODEL_944581cf89ad4e0ab91fc0995276398a"
     }
    },
    "65c919673eda4418b807b54c578d8e3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efe60a0509c64bea92308068d8d9034a",
      "placeholder": "​",
      "style": "IPY_MODEL_a7bd8b3ad2d64c00887e9da00f2e092b",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "1b6af8a1c31c4696bd34d6974ff7e09f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b52b7c854364057b49c3d175da3e6a5",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b4c659a053204483be654a6499ad741f",
      "value": 2
     }
    },
    "ed1416c394e14c70b88171574c43291e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d994e406b504632a4be188664f3bc9d",
      "placeholder": "​",
      "style": "IPY_MODEL_5649607ad89b4a78b4d69af2a6475c74",
      "value": " 2/2 [00:02&lt;00:00,  1.03s/it]"
     }
    },
    "944581cf89ad4e0ab91fc0995276398a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efe60a0509c64bea92308068d8d9034a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7bd8b3ad2d64c00887e9da00f2e092b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b52b7c854364057b49c3d175da3e6a5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4c659a053204483be654a6499ad741f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d994e406b504632a4be188664f3bc9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5649607ad89b4a78b4d69af2a6475c74": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
